{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: The Geometry of Meaning\n",
    "## Training Word2Vec Skip-gram from Scratch\n",
    "\n",
    "**Goal:** Build a Skip-gram model and visualize the learned embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# Download TinyShakespeare\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "try:\n",
    "    urllib.request.urlretrieve(url, \"tinyshakespeare.txt\")\n",
    "    print(\"Downloaded TinyShakespeare corpus\")\n",
    "except:\n",
    "    print(\"Using local file or sample data\")\n",
    "\n",
    "# Load and preprocess\n",
    "with open(\"tinyshakespeare.txt\", \"r\") as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "# Simple tokenization (word-level)\n",
    "words = text.split()\n",
    "print(f\"Total words: {len(words)}\")\n",
    "print(f\"Unique words: {len(set(words))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(words: List[str], min_count: int = 5) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Create word-to-index and index-to-word mappings.\n",
    "    \n",
    "    Args:\n",
    "        words: List of words\n",
    "        min_count: Minimum frequency to include word\n",
    "        \n",
    "    Returns:\n",
    "        word2idx: Word to index mapping\n",
    "        idx2word: Index to word mapping\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    # Filter words by minimum count\n",
    "    vocab_words = [w for w, c in word_counts.items() if c >= min_count]\n",
    "    \n",
    "    # Create mappings\n",
    "    word2idx = {w: i for i, w in enumerate(vocab_words)}\n",
    "    idx2word = {i: w for w, i in word2idx.items()}\n",
    "    \n",
    "    return word2idx, idx2word\n",
    "\n",
    "word2idx, idx2word = build_vocabulary(words, min_count=5)\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Sample words: {list(word2idx.keys())[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Create Skip-gram Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_skipgram_pairs(words: List[str], word2idx: Dict[str, int], window_size: int = 2) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Generate (center_word, context_word) pairs for Skip-gram training.\n",
    "    \n",
    "    Args:\n",
    "        words: Tokenized corpus\n",
    "        word2idx: Word to index mapping\n",
    "        window_size: Context window size (words on each side)\n",
    "        \n",
    "    Returns:\n",
    "        List of (center_idx, context_idx) pairs\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pairs = []\n",
    "    \n",
    "    for i, center_word in enumerate(words):\n",
    "        # Skip words not in vocabulary\n",
    "        if center_word not in word2idx:\n",
    "            continue\n",
    "            \n",
    "        center_idx = word2idx[center_word]\n",
    "        \n",
    "        # Get context words within window\n",
    "        for j in range(max(0, i - window_size), min(len(words), i + window_size + 1)):\n",
    "            if i == j:  # Skip center word itself\n",
    "                continue\n",
    "                \n",
    "            context_word = words[j]\n",
    "            if context_word in word2idx:\n",
    "                context_idx = word2idx[context_word]\n",
    "                pairs.append((center_idx, context_idx))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Create training pairs\n",
    "skipgram_pairs = create_skipgram_pairs(words, word2idx, window_size=2)\n",
    "print(f\"Total training pairs: {len(skipgram_pairs)}\")\n",
    "print(f\"Sample pairs: {skipgram_pairs[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, pairs: List[Tuple[int, int]]):\n",
    "        self.pairs = pairs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        center, context = self.pairs[idx]\n",
    "        return torch.tensor(center, dtype=torch.long), torch.tensor(context, dtype=torch.long)\n",
    "\n",
    "dataset = SkipGramDataset(skipgram_pairs)\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "print(f\"Number of batches: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Implement Skip-gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        \"\"\"\n",
    "        Simple Skip-gram model.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Number of unique words\n",
    "            embedding_dim: Dimension of embedding vectors\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        # Hint: You need:\n",
    "        # 1. Embedding layer for input words\n",
    "        # 2. Linear layer to predict context words\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, center_word: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            center_word: Tensor of center word indices [batch_size]\n",
    "            \n",
    "        Returns:\n",
    "            Logits for context word prediction [batch_size, vocab_size]\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # 1. Get embeddings for center words\n",
    "        # 2. Pass through output layer\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def get_embeddings(self) -> np.ndarray:\n",
    "        \"\"\"Return the learned word embeddings as numpy array.\"\"\"\n",
    "        return self.embeddings.weight.detach().cpu().numpy()\n",
    "\n",
    "# Initialize model\n",
    "embedding_dim = 128\n",
    "model = SkipGramModel(vocab_size, embedding_dim)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_skipgram(model, dataloader, epochs=5, lr=0.01):\n",
    "    \"\"\"\n",
    "    Train Skip-gram model.\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for center, context in dataloader:\n",
    "            # YOUR CODE HERE\n",
    "            # 1. Forward pass\n",
    "            # 2. Compute loss\n",
    "            # 3. Backward pass\n",
    "            # 4. Update weights\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward\n",
    "            logits = model(center)\n",
    "            loss = criterion(logits, context)\n",
    "            \n",
    "            # Backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Train model\n",
    "losses = train_skipgram(model, dataloader, epochs=10, lr=0.01)\n",
    "\n",
    "# Plot loss curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Skip-gram Training Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Visualize Embeddings\n",
    "\n",
    "### Task 1: Cosine Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cosine_matrix(embeddings: np.ndarray, words: List[str], top_n: int = 50):\n",
    "    \"\"\"\n",
    "    Plot heatmap of cosine similarities between words.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # 1. Compute pairwise cosine similarities\n",
    "    # 2. Create heatmap\n",
    "    \n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    # Select top_n most frequent words\n",
    "    selected_embeddings = embeddings[:top_n]\n",
    "    selected_words = words[:top_n]\n",
    "    \n",
    "    # Compute cosine similarity matrix\n",
    "    cos_sim = cosine_similarity(selected_embeddings)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cos_sim, xticklabels=selected_words, yticklabels=selected_words, cmap='coolwarm')\n",
    "    plt.title('Cosine Similarity Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get embeddings and word list\n",
    "embeddings = model.get_embeddings()\n",
    "word_list = [idx2word[i] for i in range(len(idx2word))]\n",
    "\n",
    "plot_cosine_matrix(embeddings, word_list, top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: 2D Projection with t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings_2d(embeddings: np.ndarray, words: List[str], method='tsne', top_n: int = 200):\n",
    "    \"\"\"\n",
    "    Project embeddings to 2D and visualize.\n",
    "    \n",
    "    Args:\n",
    "        method: 'pca' or 'tsne'\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Select subset\n",
    "    selected_embeddings = embeddings[:top_n]\n",
    "    selected_words = words[:top_n]\n",
    "    \n",
    "    # Dimensionality reduction\n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=2)\n",
    "    else:\n",
    "        reducer = TSNE(n_components=2, random_state=42)\n",
    "    \n",
    "    coords = reducer.fit_transform(selected_embeddings)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    plt.scatter(coords[:, 0], coords[:, 1], alpha=0.5)\n",
    "    \n",
    "    # Annotate points\n",
    "    for i, word in enumerate(selected_words):\n",
    "        plt.annotate(word, (coords[i, 0], coords[i, 1]), fontsize=8, alpha=0.7)\n",
    "    \n",
    "    plt.title(f'Word Embeddings ({method.upper()})')\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_embeddings_2d(embeddings, word_list, method='tsne', top_n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Test Semantic Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_analogy(embeddings, word2idx, idx2word, a: str, b: str, c: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Test analogy: a is to b as c is to ?\n",
    "    \n",
    "    Example: king - man + woman â‰ˆ queen\n",
    "    \n",
    "    Returns top_k most similar words to (b - a + c)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "    # Get embeddings for input words\n",
    "    if a not in word2idx or b not in word2idx or c not in word2idx:\n",
    "        print(f\"One or more words not in vocabulary\")\n",
    "        return\n",
    "    \n",
    "    a_vec = embeddings[word2idx[a]]\n",
    "    b_vec = embeddings[word2idx[b]]\n",
    "    c_vec = embeddings[word2idx[c]]\n",
    "    \n",
    "    # Compute target vector: b - a + c\n",
    "    target_vec = b_vec - a_vec + c_vec\n",
    "    \n",
    "    # Find most similar words\n",
    "    similarities = cosine_similarity([target_vec], embeddings)[0]\n",
    "    \n",
    "    # Get top-k (excluding input words)\n",
    "    top_indices = similarities.argsort()[::-1]\n",
    "    \n",
    "    print(f\"\\n'{a}' is to '{b}' as '{c}' is to:\")\n",
    "    count = 0\n",
    "    for idx in top_indices:\n",
    "        word = idx2word[idx]\n",
    "        if word not in [a, b, c] and count < top_k:\n",
    "            print(f\"  {count+1}. {word} (similarity: {similarities[idx]:.4f})\")\n",
    "            count += 1\n",
    "        if count >= top_k:\n",
    "            break\n",
    "\n",
    "# Test some analogies (these may not work well on small corpus)\n",
    "test_analogy(embeddings, word2idx, idx2word, 'king', 'queen', 'man')\n",
    "test_analogy(embeddings, word2idx, idx2word, 'good', 'better', 'bad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Compare One-Hot vs Learned Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotModel(nn.Module):\n",
    "    \"\"\"Baseline model using one-hot encoding.\"\"\"\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "        # No embedding layer - input is already one-hot\n",
    "        self.output = nn.Linear(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, center_word_onehot: torch.Tensor) -> torch.Tensor:\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "\n",
    "# Compare model sizes\n",
    "onehot_model = OneHotModel(vocab_size)\n",
    "\n",
    "skipgram_params = sum(p.numel() for p in model.parameters())\n",
    "onehot_params = sum(p.numel() for p in onehot_model.parameters())\n",
    "\n",
    "print(f\"\\nModel Size Comparison:\")\n",
    "print(f\"Skip-gram (embedding_dim={embedding_dim}): {skipgram_params:,} parameters\")\n",
    "print(f\"One-hot (no compression): {onehot_params:,} parameters\")\n",
    "print(f\"Memory savings: {(1 - skipgram_params/onehot_params)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Analysis and Reflection\n",
    "\n",
    "### Questions to Answer:\n",
    "\n",
    "1. **Why are one-hot vectors orthogonal?**\n",
    "   - YOUR ANSWER HERE\n",
    "\n",
    "2. **What does cosine similarity measure in embedding space?**\n",
    "   - YOUR ANSWER HERE\n",
    "\n",
    "3. **Did the analogies work? Why or why not?**\n",
    "   - YOUR ANSWER HERE (Hint: corpus size matters!)\n",
    "\n",
    "4. **What clusters did you observe in the t-SNE plot?**\n",
    "   - YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Completion Checklist\n",
    "\n",
    "- [ ] Built vocabulary from corpus\n",
    "- [ ] Created Skip-gram training pairs\n",
    "- [ ] Implemented `SkipGramModel`\n",
    "- [ ] Trained model and plotted loss curve\n",
    "- [ ] Visualized cosine similarity matrix\n",
    "- [ ] Created 2D projection (t-SNE/PCA)\n",
    "- [ ] Tested semantic analogies\n",
    "- [ ] Compared with one-hot baseline\n",
    "- [ ] Answered reflection questions\n",
    "\n",
    "## ðŸš€ Next Project\n",
    "Move to **03_rope_animator** to learn how to add positional information to sequences!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
