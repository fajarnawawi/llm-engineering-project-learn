{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: The Geometry of Meaning - SOLUTION\n",
    "## Training Word2Vec Skip-gram from Scratch\n",
    "\n",
    "**This notebook contains complete solutions to all tasks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Download TinyShakespeare\n",
    "import urllib.request\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "try:\n",
    "    urllib.request.urlretrieve(url, \"tinyshakespeare.txt\")\n",
    "    print(\"Downloaded TinyShakespeare corpus\")\n",
    "    with open(\"tinyshakespeare.txt\", \"r\") as f:\n",
    "        text = f.read().lower()\n",
    "except:\n",
    "    print(\"Using sample data\")\n",
    "    text = \"the quick brown fox jumps over the lazy dog \" * 100\n",
    "\n",
    "# Simple tokenization (word-level)\n",
    "words = text.split()\n",
    "print(f\"Total words: {len(words)}\")\n",
    "print(f\"Unique words: {len(set(words))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(words: List[str], min_count: int = 5) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Create word-to-index and index-to-word mappings.\n",
    "    \n",
    "    Args:\n",
    "        words: List of words\n",
    "        min_count: Minimum frequency to include word\n",
    "        \n",
    "    Returns:\n",
    "        word2idx: Word to index mapping\n",
    "        idx2word: Index to word mapping\n",
    "    \"\"\"\n",
    "    word_counts = Counter(words)\n",
    "    \n",
    "    # Filter words by minimum count\n",
    "    vocab_words = [w for w, c in word_counts.items() if c >= min_count]\n",
    "    \n",
    "    # Create mappings\n",
    "    word2idx = {w: i for i, w in enumerate(vocab_words)}\n",
    "    idx2word = {i: w for w, i in word2idx.items()}\n",
    "    \n",
    "    return word2idx, idx2word\n",
    "\n",
    "word2idx, idx2word = build_vocabulary(words, min_count=5)\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Sample words: {list(word2idx.keys())[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Create Skip-gram Training Data - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_skipgram_pairs(words: List[str], word2idx: Dict[str, int], window_size: int = 2) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Generate (center_word, context_word) pairs for Skip-gram training.\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    for i, center_word in enumerate(words):\n",
    "        # Skip words not in vocabulary\n",
    "        if center_word not in word2idx:\n",
    "            continue\n",
    "            \n",
    "        center_idx = word2idx[center_word]\n",
    "        \n",
    "        # Get context words within window\n",
    "        for j in range(max(0, i - window_size), min(len(words), i + window_size + 1)):\n",
    "            if i == j:  # Skip center word itself\n",
    "                continue\n",
    "                \n",
    "            context_word = words[j]\n",
    "            if context_word in word2idx:\n",
    "                context_idx = word2idx[context_word]\n",
    "                pairs.append((center_idx, context_idx))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Create training pairs\n",
    "skipgram_pairs = create_skipgram_pairs(words, word2idx, window_size=2)\n",
    "print(f\"Total training pairs: {len(skipgram_pairs)}\")\n",
    "print(f\"Sample pairs (as indices): {skipgram_pairs[:5]}\")\n",
    "print(f\"Sample pairs (as words): {[(idx2word[c], idx2word[ctx]) for c, ctx in skipgram_pairs[:5]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create PyTorch Dataset - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, pairs: List[Tuple[int, int]]):\n",
    "        self.pairs = pairs\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        center, context = self.pairs[idx]\n",
    "        return torch.tensor(center, dtype=torch.long), torch.tensor(context, dtype=torch.long)\n",
    "\n",
    "dataset = SkipGramDataset(skipgram_pairs)\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "print(f\"Number of batches: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Implement Skip-gram Model - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int):\n",
    "        \"\"\"\n",
    "        Simple Skip-gram model.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Number of unique words\n",
    "            embedding_dim: Dimension of embedding vectors\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embedding layer for input words\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Output layer to predict context words\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, center_word: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            center_word: Tensor of center word indices [batch_size]\n",
    "            \n",
    "        Returns:\n",
    "            Logits for context word prediction [batch_size, vocab_size]\n",
    "        \"\"\"\n",
    "        # Get embeddings for center words\n",
    "        embeds = self.embeddings(center_word)  # [batch_size, embedding_dim]\n",
    "        \n",
    "        # Pass through output layer\n",
    "        logits = self.output(embeds)  # [batch_size, vocab_size]\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def get_embeddings(self) -> np.ndarray:\n",
    "        \"\"\"Return the learned word embeddings as numpy array.\"\"\"\n",
    "        return self.embeddings.weight.detach().cpu().numpy()\n",
    "\n",
    "# Initialize model\n",
    "embedding_dim = 128\n",
    "model = SkipGramModel(vocab_size, embedding_dim)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Train the Model - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_skipgram(model, dataloader, epochs=5, lr=0.01):\n",
    "    \"\"\"\n",
    "    Train Skip-gram model.\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for center, context in dataloader:\n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(center)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(logits, context)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Train model\n",
    "print(\"Training Skip-gram model...\\n\")\n",
    "losses = train_skipgram(model, dataloader, epochs=10, lr=0.01)\n",
    "\n",
    "# Plot loss curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses, marker='o')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Skip-gram Training Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Visualize Embeddings - SOLUTION\n",
    "\n",
    "### Task 1: Cosine Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cosine_matrix(embeddings: np.ndarray, words: List[str], top_n: int = 50):\n",
    "    \"\"\"\n",
    "    Plot heatmap of cosine similarities between words.\n",
    "    \"\"\"\n",
    "    # Select top_n most frequent words\n",
    "    selected_embeddings = embeddings[:top_n]\n",
    "    selected_words = words[:top_n]\n",
    "    \n",
    "    # Compute cosine similarity matrix\n",
    "    cos_sim = cosine_similarity(selected_embeddings)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cos_sim, \n",
    "                xticklabels=selected_words, \n",
    "                yticklabels=selected_words, \n",
    "                cmap='coolwarm',\n",
    "                center=0,\n",
    "                vmin=-1,\n",
    "                vmax=1)\n",
    "    plt.title('Cosine Similarity Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Get embeddings and word list\n",
    "embeddings = model.get_embeddings()\n",
    "word_list = [idx2word[i] for i in range(len(idx2word))]\n",
    "\n",
    "plot_cosine_matrix(embeddings, word_list, top_n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: 2D Projection with t-SNE - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings_2d(embeddings: np.ndarray, words: List[str], method='tsne', top_n: int = 200):\n",
    "    \"\"\"\n",
    "    Project embeddings to 2D and visualize.\n",
    "    \n",
    "    Args:\n",
    "        method: 'pca' or 'tsne'\n",
    "    \"\"\"\n",
    "    # Select subset\n",
    "    selected_embeddings = embeddings[:top_n]\n",
    "    selected_words = words[:top_n]\n",
    "    \n",
    "    # Dimensionality reduction\n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=2)\n",
    "        title_suffix = 'PCA'\n",
    "    else:\n",
    "        reducer = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "        title_suffix = 't-SNE'\n",
    "    \n",
    "    coords = reducer.fit_transform(selected_embeddings)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    plt.scatter(coords[:, 0], coords[:, 1], alpha=0.5, s=50)\n",
    "    \n",
    "    # Annotate points (only every 3rd to avoid clutter)\n",
    "    for i in range(0, len(selected_words), 3):\n",
    "        plt.annotate(selected_words[i], \n",
    "                    (coords[i, 0], coords[i, 1]), \n",
    "                    fontsize=8, \n",
    "                    alpha=0.7)\n",
    "    \n",
    "    plt.title(f'Word Embeddings ({title_suffix})')\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Generating 2D visualization (this may take a minute)...\")\n",
    "plot_embeddings_2d(embeddings, word_list, method='tsne', top_n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Test Semantic Analogies - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_analogy(embeddings, word2idx, idx2word, a: str, b: str, c: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Test analogy: a is to b as c is to ?\n",
    "    \n",
    "    Example: king - man + woman â‰ˆ queen\n",
    "    \n",
    "    Returns top_k most similar words to (b - a + c)\n",
    "    \"\"\"\n",
    "    # Get embeddings for input words\n",
    "    if a not in word2idx or b not in word2idx or c not in word2idx:\n",
    "        print(f\"One or more words not in vocabulary\")\n",
    "        return\n",
    "    \n",
    "    a_vec = embeddings[word2idx[a]]\n",
    "    b_vec = embeddings[word2idx[b]]\n",
    "    c_vec = embeddings[word2idx[c]]\n",
    "    \n",
    "    # Compute target vector: b - a + c\n",
    "    target_vec = b_vec - a_vec + c_vec\n",
    "    \n",
    "    # Find most similar words\n",
    "    similarities = cosine_similarity([target_vec], embeddings)[0]\n",
    "    \n",
    "    # Get top-k (excluding input words)\n",
    "    top_indices = similarities.argsort()[::-1]\n",
    "    \n",
    "    print(f\"\\n'{a}' is to '{b}' as '{c}' is to:\")\n",
    "    count = 0\n",
    "    for idx in top_indices:\n",
    "        word = idx2word[idx]\n",
    "        if word not in [a, b, c] and count < top_k:\n",
    "            print(f\"  {count+1}. {word} (similarity: {similarities[idx]:.4f})\")\n",
    "            count += 1\n",
    "        if count >= top_k:\n",
    "            break\n",
    "\n",
    "# Test some analogies\n",
    "# Note: These may not work well on small corpus - Shakespeare doesn't have \"king/queen\" examples\n",
    "# But we can test with words that appear in the text\n",
    "\n",
    "print(\"Testing semantic analogies...\")\n",
    "print(\"Note: Results depend on corpus content and may not always be meaningful.\")\n",
    "\n",
    "# Find some common words to test\n",
    "common_words = list(word2idx.keys())[:50]\n",
    "print(f\"\\nCommon words in vocabulary: {common_words[:20]}\")\n",
    "\n",
    "# Try some analogies if appropriate words exist\n",
    "if 'good' in word2idx and 'better' in word2idx and 'bad' in word2idx:\n",
    "    test_analogy(embeddings, word2idx, idx2word, 'good', 'better', 'bad')\n",
    "\n",
    "if 'man' in word2idx and 'king' in word2idx and 'woman' in word2idx:\n",
    "    test_analogy(embeddings, word2idx, idx2word, 'man', 'king', 'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Compare One-Hot vs Learned Embeddings - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotModel(nn.Module):\n",
    "    \"\"\"Baseline model using one-hot encoding.\"\"\"\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super().__init__()\n",
    "        # No embedding layer - input is already one-hot\n",
    "        # Direct mapping from vocab_size to vocab_size\n",
    "        self.output = nn.Linear(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, center_word_indices: torch.Tensor) -> torch.Tensor:\n",
    "        # Convert indices to one-hot\n",
    "        batch_size = center_word_indices.size(0)\n",
    "        one_hot = torch.zeros(batch_size, self.output.in_features)\n",
    "        one_hot.scatter_(1, center_word_indices.unsqueeze(1), 1)\n",
    "        \n",
    "        # Pass through linear layer\n",
    "        return self.output(one_hot)\n",
    "\n",
    "# Compare model sizes\n",
    "onehot_model = OneHotModel(vocab_size)\n",
    "\n",
    "skipgram_params = sum(p.numel() for p in model.parameters())\n",
    "onehot_params = sum(p.numel() for p in onehot_model.parameters())\n",
    "\n",
    "print(f\"\\nModel Size Comparison:\")\n",
    "print(f\"=\"*50)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "print(f\"\\nSkip-gram (with embeddings):\")\n",
    "print(f\"  Parameters: {skipgram_params:,}\")\n",
    "print(f\"  Memory: ~{skipgram_params * 4 / 1024:.2f} KB (FP32)\")\n",
    "print(f\"\\nOne-hot (no compression):\")\n",
    "print(f\"  Parameters: {onehot_params:,}\")\n",
    "print(f\"  Memory: ~{onehot_params * 4 / 1024:.2f} KB (FP32)\")\n",
    "print(f\"\\nMemory savings: {(1 - skipgram_params/onehot_params)*100:.1f}%\")\n",
    "print(f\"Compression ratio: {onehot_params / skipgram_params:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Analysis and Reflection\n",
    "\n",
    "### Questions and Answers:\n",
    "\n",
    "#### 1. Why are one-hot vectors orthogonal?\n",
    "\n",
    "**Answer:** \n",
    "One-hot vectors have exactly one element set to 1 and all others to 0. When computing the dot product of two different one-hot vectors:\n",
    "- `[1,0,0,0] Â· [0,1,0,0] = 0`\n",
    "- The dot product is always 0 (orthogonal)\n",
    "- This means one-hot encoding captures NO semantic similarity between words\n",
    "- \"cat\" and \"dog\" are as different as \"cat\" and \"computer\"\n",
    "\n",
    "#### 2. What does cosine similarity measure in embedding space?\n",
    "\n",
    "**Answer:**\n",
    "Cosine similarity measures the angle between two vectors:\n",
    "- cos(Î¸) = 1: Vectors point in the same direction (very similar meaning)\n",
    "- cos(Î¸) = 0: Vectors are orthogonal (unrelated)\n",
    "- cos(Î¸) = -1: Vectors point in opposite directions (opposite meaning)\n",
    "\n",
    "In embedding space:\n",
    "- Words with similar contexts get similar embeddings\n",
    "- High cosine similarity = words are semantically related\n",
    "- Examples: \"king\" and \"queen\", \"run\" and \"running\"\n",
    "\n",
    "#### 3. Did the analogies work? Why or why not?\n",
    "\n",
    "**Answer:**\n",
    "Analogies may or may not work well depending on:\n",
    "- **Corpus size**: TinyShakespeare is small (~1MB). Analogies work better with larger corpora\n",
    "- **Training time**: More epochs = better embeddings\n",
    "- **Word frequency**: Rare words have poor embeddings\n",
    "- **Context diversity**: Words need to appear in varied contexts\n",
    "\n",
    "The famous \"king - man + woman = queen\" works on large corpora (Wikipedia, Common Crawl) but may fail on small domain-specific text.\n",
    "\n",
    "#### 4. What clusters did you observe in the t-SNE plot?\n",
    "\n",
    "**Answer:**\n",
    "Expected clusters (depends on corpus):\n",
    "- **Syntactic clusters**: Verbs group together, nouns group together\n",
    "- **Semantic clusters**: Related concepts (characters, emotions, actions)\n",
    "- **Functional words**: \"the\", \"and\", \"of\" may cluster separately\n",
    "- **Character names**: In Shakespeare, character names may cluster by play or role\n",
    "\n",
    "The geometry of embeddings captures both semantic and syntactic relationships!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Additional Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find nearest neighbors for a word\n",
    "def find_nearest_neighbors(word: str, embeddings, word2idx, idx2word, top_k=10):\n",
    "    \"\"\"Find most similar words to a given word.\"\"\"\n",
    "    if word not in word2idx:\n",
    "        print(f\"'{word}' not in vocabulary\")\n",
    "        return\n",
    "    \n",
    "    word_vec = embeddings[word2idx[word]]\n",
    "    similarities = cosine_similarity([word_vec], embeddings)[0]\n",
    "    \n",
    "    # Get top-k\n",
    "    top_indices = similarities.argsort()[::-1][1:top_k+1]  # Exclude the word itself\n",
    "    \n",
    "    print(f\"\\nNearest neighbors of '{word}':\")\n",
    "    for i, idx in enumerate(top_indices, 1):\n",
    "        print(f\"  {i}. {idx2word[idx]} (similarity: {similarities[idx]:.4f})\")\n",
    "\n",
    "# Test with some words\n",
    "test_words = ['love', 'death', 'king', 'good']\n",
    "for word in test_words:\n",
    "    if word in word2idx:\n",
    "        find_nearest_neighbors(word, embeddings, word2idx, idx2word, top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Completion Checklist\n",
    "\n",
    "- âœ… Built vocabulary from corpus\n",
    "- âœ… Created Skip-gram training pairs\n",
    "- âœ… Implemented `SkipGramModel`\n",
    "- âœ… Trained model and plotted loss curve\n",
    "- âœ… Visualized cosine similarity matrix\n",
    "- âœ… Created 2D projection (t-SNE/PCA)\n",
    "- âœ… Tested semantic analogies\n",
    "- âœ… Compared with one-hot baseline\n",
    "- âœ… Answered reflection questions\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Dense embeddings beat one-hot**: Capture semantic relationships\n",
    "2. **Context is key**: Words with similar contexts get similar embeddings\n",
    "3. **Geometry encodes meaning**: Vector arithmetic reveals semantic relationships\n",
    "4. **Corpus matters**: Quality and size of training data affect results\n",
    "\n",
    "## ðŸš€ Next Project\n",
    "Move to **03_rope_animator** to learn how to add positional information to sequences!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
