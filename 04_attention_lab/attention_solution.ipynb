{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: The Attention Surgeon - SOLUTION\n",
    "## Building Scaled Dot-Product Attention from Scratch\n",
    "\n",
    "**This notebook contains complete solutions to all tasks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implement Core Attention Function - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(Q: torch.Tensor,\n",
    "              K: torch.Tensor,\n",
    "              V: torch.Tensor,\n",
    "              mask: Optional[torch.Tensor] = None,\n",
    "              scale: bool = True,\n",
    "              return_weights: bool = True) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention - COMPLETE IMPLEMENTATION\n",
    "    \n",
    "    Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V\n",
    "    \n",
    "    Args:\n",
    "        Q: Query [batch, seq_len, d_k]\n",
    "        K: Key [batch, seq_len, d_k]\n",
    "        V: Value [batch, seq_len, d_v]\n",
    "        mask: Optional mask [batch, seq_len, seq_len] or [seq_len, seq_len]\n",
    "        scale: Whether to apply 1/âˆšd_k scaling\n",
    "        return_weights: Whether to return attention weights\n",
    "        \n",
    "    Returns:\n",
    "        output: [batch, seq_len, d_v]\n",
    "        attention_weights: [batch, seq_len, seq_len] (if return_weights=True)\n",
    "    \"\"\"\n",
    "    # Step 1: Compute Q @ K^T\n",
    "    scores = torch.bmm(Q, K.transpose(1, 2))  # [batch, seq_len, seq_len]\n",
    "    \n",
    "    # Step 2: Scale by 1/âˆšd_k\n",
    "    if scale:\n",
    "        d_k = Q.size(-1)\n",
    "        scores = scores / np.sqrt(d_k)\n",
    "    \n",
    "    # Step 3: Apply mask (set masked positions to -inf)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    \n",
    "    # Step 4: Apply softmax\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Handle NaN (occurs when entire row is masked)\n",
    "    attention_weights = attention_weights.masked_fill(torch.isnan(attention_weights), 0.0)\n",
    "    \n",
    "    # Step 5: Multiply by V\n",
    "    output = torch.bmm(attention_weights, V)\n",
    "    \n",
    "    if return_weights:\n",
    "        return output, attention_weights\n",
    "    else:\n",
    "        return output, None\n",
    "\n",
    "# Test the attention function\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_k = 8\n",
    "d_v = 8\n",
    "\n",
    "Q = torch.randn(batch_size, seq_len, d_k)\n",
    "K = torch.randn(batch_size, seq_len, d_k)\n",
    "V = torch.randn(batch_size, seq_len, d_v)\n",
    "\n",
    "output, weights = attention(Q, K, V)\n",
    "\n",
    "print(f\"Input shapes:\")\n",
    "print(f\"  Q: {Q.shape}\")\n",
    "print(f\"  K: {K.shape}\")\n",
    "print(f\"  V: {V.shape}\")\n",
    "print(f\"\\nOutput shapes:\")\n",
    "print(f\"  Output: {output.shape}\")\n",
    "print(f\"  Attention weights: {weights.shape}\")\n",
    "print(f\"\\nAttention weights sum (should be ~1.0 per row):\")\n",
    "print(f\"  {weights[0].sum(dim=-1)}\")\n",
    "print(\"\\nâœ“ Attention implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Visualize Attention Patterns - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_heatmap(attention_weights: torch.Tensor,\n",
    "                          tokens: List[str],\n",
    "                          title: str = \"Attention Weights\"):\n",
    "    \"\"\"\n",
    "    Plot attention weight matrix as heatmap.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Convert to numpy\n",
    "    if isinstance(attention_weights, torch.Tensor):\n",
    "        attention_weights = attention_weights.detach().cpu().numpy()\n",
    "    \n",
    "    # Plot heatmap\n",
    "    sns.heatmap(attention_weights, \n",
    "                xticklabels=tokens, \n",
    "                yticklabels=tokens,\n",
    "                cmap='viridis',\n",
    "                annot=True,\n",
    "                fmt='.2f',\n",
    "                cbar_kws={'label': 'Attention Weight'})\n",
    "    \n",
    "    plt.xlabel('Key / Value')\n",
    "    plt.ylabel('Query')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create sample tokens and attention\n",
    "tokens = ['The', 'cat', 'sat', 'on', 'mat']\n",
    "sample_attention = weights[0]  # Take first batch\n",
    "\n",
    "plot_attention_heatmap(sample_attention, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Experiment with Scaling Factor - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_scaling_factor(d_k_values=[16, 64, 256, 1024]):\n",
    "    \"\"\"\n",
    "    Test impact of scaling factor across different d_k values.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'd_k': [],\n",
    "        'with_scale_max': [],\n",
    "        'without_scale_max': [],\n",
    "        'with_scale_entropy': [],\n",
    "        'without_scale_entropy': []\n",
    "    }\n",
    "    \n",
    "    for d_k in d_k_values:\n",
    "        # Create random Q, K, V\n",
    "        Q = torch.randn(1, 10, d_k)\n",
    "        K = torch.randn(1, 10, d_k)\n",
    "        V = torch.randn(1, 10, d_k)\n",
    "        \n",
    "        # With scaling\n",
    "        _, weights_scaled = attention(Q, K, V, scale=True)\n",
    "        \n",
    "        # Without scaling\n",
    "        _, weights_unscaled = attention(Q, K, V, scale=False)\n",
    "        \n",
    "        # Compute statistics\n",
    "        results['d_k'].append(d_k)\n",
    "        results['with_scale_max'].append(weights_scaled.max().item())\n",
    "        results['without_scale_max'].append(weights_unscaled.max().item())\n",
    "        \n",
    "        # Compute entropy (measure of distribution sharpness)\n",
    "        def compute_entropy(w):\n",
    "            w = w + 1e-9  # Avoid log(0)\n",
    "            return -(w * torch.log(w)).sum(dim=-1).mean().item()\n",
    "        \n",
    "        results['with_scale_entropy'].append(compute_entropy(weights_scaled))\n",
    "        results['without_scale_entropy'].append(compute_entropy(weights_unscaled))\n",
    "    \n",
    "    # Plot results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Max attention weight\n",
    "    ax1.plot(results['d_k'], results['with_scale_max'], marker='o', label='With Scaling', linewidth=2)\n",
    "    ax1.plot(results['d_k'], results['without_scale_max'], marker='s', label='Without Scaling', linewidth=2)\n",
    "    ax1.axhline(y=1.0, color='r', linestyle='--', alpha=0.5, label='Saturation (1.0)')\n",
    "    ax1.set_xlabel('d_k (Key Dimension)', fontsize=12)\n",
    "    ax1.set_ylabel('Max Attention Weight', fontsize=12)\n",
    "    ax1.set_title('Softmax Saturation vs d_k', fontsize=14)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xscale('log')\n",
    "    \n",
    "    # Entropy\n",
    "    ax2.plot(results['d_k'], results['with_scale_entropy'], marker='o', label='With Scaling', linewidth=2)\n",
    "    ax2.plot(results['d_k'], results['without_scale_entropy'], marker='s', label='Without Scaling', linewidth=2)\n",
    "    ax2.set_xlabel('d_k (Key Dimension)', fontsize=12)\n",
    "    ax2.set_ylabel('Entropy (nats)', fontsize=12)\n",
    "    ax2.set_title('Attention Entropy vs d_k', fontsize=14)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print analysis\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ANALYSIS: Why Scaling Matters\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nWithout scaling:\")\n",
    "    print(\"  - As d_k increases, QK^T values grow (variance ~ d_k)\")\n",
    "    print(\"  - Softmax becomes peaked (max â†’ 1.0)\")\n",
    "    print(\"  - Gradients vanish in saturated regions\")\n",
    "    print(\"  - Entropy decreases (less diverse attention)\")\n",
    "    print(\"\\nWith scaling (1/âˆšd_k):\")\n",
    "    print(\"  - QK^T values normalized\")\n",
    "    print(\"  - Softmax remains balanced\")\n",
    "    print(\"  - Gradients flow properly\")\n",
    "    print(\"  - Entropy stable across different d_k\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = experiment_scaling_factor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Masking - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len: int, device='cpu') -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create lower-triangular mask for causal attention.\n",
    "    \n",
    "    Returns:\n",
    "        mask: [seq_len, seq_len] with 1s in lower triangle, 0s in upper\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n",
    "    return mask\n",
    "\n",
    "def create_reverse_mask(seq_len: int, device='cpu') -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    WRONG: Create upper-triangular mask (lets model cheat!).\n",
    "    \"\"\"\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len, device=device))\n",
    "    return mask\n",
    "\n",
    "# Visualize masks\n",
    "seq_len = 8\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "reverse_mask = create_reverse_mask(seq_len)\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Causal mask\n",
    "sns.heatmap(causal_mask.numpy(), ax=ax1, cmap='Blues', cbar=False, square=True, annot=True, fmt='.0f')\n",
    "ax1.set_title('Causal Mask (Correct)\\nCan see past and present', fontsize=12)\n",
    "ax1.set_xlabel('Key Position')\n",
    "ax1.set_ylabel('Query Position')\n",
    "\n",
    "# Reverse mask\n",
    "sns.heatmap(reverse_mask.numpy(), ax=ax2, cmap='Reds', cbar=False, square=True, annot=True, fmt='.0f')\n",
    "ax2.set_title('Reverse Mask (Wrong!)\\nCan see future!', fontsize=12)\n",
    "ax2.set_xlabel('Key Position')\n",
    "ax2.set_ylabel('Query Position')\n",
    "\n",
    "# No mask\n",
    "no_mask = torch.ones(seq_len, seq_len)\n",
    "sns.heatmap(no_mask.numpy(), ax=ax3, cmap='Greens', cbar=False, square=True, annot=True, fmt='.0f')\n",
    "ax3.set_title('No Mask (Bidirectional)\\nCan see everything', fontsize=12)\n",
    "ax3.set_xlabel('Key Position')\n",
    "ax3.set_ylabel('Query Position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMasking Examples:\")\n",
    "print(\"  1 = Can attend\")\n",
    "print(\"  0 = Cannot attend (masked out)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Causal Masking - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test attention with causal mask\n",
    "seq_len = 6\n",
    "d_k = 8\n",
    "\n",
    "Q = torch.randn(1, seq_len, d_k)\n",
    "K = torch.randn(1, seq_len, d_k)\n",
    "V = torch.randn(1, seq_len, d_k)\n",
    "\n",
    "mask = create_causal_mask(seq_len)\n",
    "\n",
    "# Without mask\n",
    "_, weights_no_mask = attention(Q, K, V, mask=None)\n",
    "\n",
    "# With causal mask\n",
    "_, weights_masked = attention(Q, K, V, mask=mask)\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.heatmap(weights_no_mask[0].detach().numpy(), ax=ax1, cmap='viridis', annot=True, fmt='.2f', vmin=0, vmax=1)\n",
    "ax1.set_title('Attention Without Mask (Bidirectional)')\n",
    "ax1.set_xlabel('Key Position')\n",
    "ax1.set_ylabel('Query Position')\n",
    "\n",
    "sns.heatmap(weights_masked[0].detach().numpy(), ax=ax2, cmap='viridis', annot=True, fmt='.2f', vmin=0, vmax=1)\n",
    "ax2.set_title('Attention With Causal Mask (Autoregressive)')\n",
    "ax2.set_xlabel('Key Position')\n",
    "ax2.set_ylabel('Query Position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Notice: With causal mask, upper triangle is all zeros!\")\n",
    "print(\"  This prevents the model from 'cheating' by seeing future tokens.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Multi-Head Attention - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention module - COMPLETE IMPLEMENTATION\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, seq_len, d_model]\n",
    "            mask: [seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.W_q(x)  # [batch, seq_len, d_model]\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        # Now: [batch, num_heads, seq_len, d_k]\n",
    "        \n",
    "        # Reshape for attention: [batch * num_heads, seq_len, d_k]\n",
    "        Q = Q.contiguous().view(batch_size * self.num_heads, seq_len, self.d_k)\n",
    "        K = K.contiguous().view(batch_size * self.num_heads, seq_len, self.d_k)\n",
    "        V = V.contiguous().view(batch_size * self.num_heads, seq_len, self.d_k)\n",
    "        \n",
    "        # Apply attention\n",
    "        output, attn_weights = attention(Q, K, V, mask=mask)\n",
    "        \n",
    "        # Reshape back: [batch, num_heads, seq_len, d_k]\n",
    "        output = output.view(batch_size, self.num_heads, seq_len, self.d_k)\n",
    "        \n",
    "        # Concatenate heads: [batch, seq_len, d_model]\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.W_o(output)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "# Test\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "x = torch.randn(2, 10, d_model)\n",
    "output, _ = mha(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of heads: {num_heads}\")\n",
    "print(f\"d_k per head: {mha.d_k}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in mha.parameters())}\")\n",
    "print(\"\\nâœ“ Multi-Head Attention implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Analysis and Reflection\n",
    "\n",
    "### Questions and Answers:\n",
    "\n",
    "#### 1. Why does increasing d_k without scaling cause softmax saturation?\n",
    "\n",
    "**Answer:**\n",
    "When Q and K are random vectors:\n",
    "- Dot product QK^T has variance proportional to d_k\n",
    "- As d_k increases, QK^T values grow larger\n",
    "- Softmax of large values â†’ nearly one-hot distribution\n",
    "- Gradients â‰ˆ 0 in saturated regions (vanishing gradients)\n",
    "- Scaling by 1/âˆšd_k normalizes the variance to ~1\n",
    "\n",
    "#### 2. What happens if we use -100 instead of -inf for masking?\n",
    "\n",
    "**Answer:**\n",
    "- softmax(-100) â‰ˆ 0, but not exactly 0\n",
    "- Small numerical errors can accumulate\n",
    "- Model might still \"leak\" tiny amounts of information\n",
    "- -inf ensures mathematically exact masking (softmax(-inf) = 0)\n",
    "\n",
    "#### 3. Can attention increase the rank of the input?\n",
    "\n",
    "**Answer:**\n",
    "No! Attention output is a weighted sum (convex combination) of V:\n",
    "- Output = Î£ attention_weights[i] * V[i]\n",
    "- Weighted sums cannot increase rank\n",
    "- rank(Output) â‰¤ rank(V)\n",
    "- This is why FFN layers are needed to increase expressiveness\n",
    "\n",
    "#### 4. Why do we need multiple heads?\n",
    "\n",
    "**Answer:**\n",
    "Multiple heads allow learning different types of relationships:\n",
    "- Head 1: Syntactic dependencies (subject-verb)\n",
    "- Head 2: Semantic similarity\n",
    "- Head 3: Positional patterns\n",
    "- Each head can specialize in different patterns\n",
    "- Analogous to multiple convolutional filters in CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Completion Checklist\n",
    "\n",
    "- âœ… Implemented `attention()` function\n",
    "- âœ… Visualized attention heatmap\n",
    "- âœ… Tested scaling factor experiment\n",
    "- âœ… Observed softmax saturation without scaling\n",
    "- âœ… Implemented causal masking\n",
    "- âœ… Visualized masked vs unmasked attention\n",
    "- âœ… Implemented Multi-Head Attention\n",
    "- âœ… Answered reflection questions\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Scaling is critical**: 1/âˆšd_k prevents softmax saturation\n",
    "2. **Masking enables causality**: -inf ensures no future information\n",
    "3. **Multiple heads = multiple perspectives**: Different relationship types\n",
    "4. **Attention is a routing mechanism**: Weighted sum, not transformation\n",
    "\n",
    "## ðŸš€ Next Project\n",
    "Move to **05_block_builder** to assemble the full Transformer block!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
