{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: The Tokenizer Smith - SOLUTION\n",
    "## Building Byte-Pair Encoding from Scratch\n",
    "\n",
    "**This notebook contains complete solutions to all tasks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample corpus for training\n",
    "SAMPLE_TEXT = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog.\n",
    "The dog was not amused by the fox's behavior.\n",
    "Jumping quickly, the fox escaped into the forest.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Corpus length: {len(SAMPLE_TEXT)} characters\")\n",
    "print(f\"Unique characters: {len(set(SAMPLE_TEXT))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implement BPE Core Functions\n",
    "\n",
    "### Task 1: Get Pair Frequencies - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_frequencies(tokens: List[str]) -> Dict[Tuple[str, str], int]:\n",
    "    \"\"\"\n",
    "    Count the frequency of all adjacent token pairs.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of current tokens\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping (token1, token2) -> frequency\n",
    "    \"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    \n",
    "    # Iterate through adjacent pairs\n",
    "    for i in range(len(tokens) - 1):\n",
    "        pair = (tokens[i], tokens[i + 1])\n",
    "        pairs[pair] += 1\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Test the function\n",
    "test_tokens = ['h', 'e', 'l', 'l', 'o']\n",
    "result = get_pair_frequencies(test_tokens)\n",
    "print(\"Test input:\", test_tokens)\n",
    "print(\"Pair frequencies:\", dict(result))\n",
    "print(\"Expected: {('h', 'e'): 1, ('e', 'l'): 1, ('l', 'l'): 1, ('l', 'o'): 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Merge a Pair - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(tokens: List[str], pair: Tuple[str, str], new_token: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Replace all occurrences of a pair with a new merged token.\n",
    "    \n",
    "    Args:\n",
    "        tokens: Current token list\n",
    "        pair: The pair to merge (token1, token2)\n",
    "        new_token: The merged token (token1 + token2)\n",
    "        \n",
    "    Returns:\n",
    "        New token list with pairs merged\n",
    "    \"\"\"\n",
    "    new_tokens = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(tokens):\n",
    "        # Check if current position matches the pair\n",
    "        if i < len(tokens) - 1 and (tokens[i], tokens[i + 1]) == pair:\n",
    "            new_tokens.append(new_token)\n",
    "            i += 2  # Skip both tokens in the pair\n",
    "        else:\n",
    "            new_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "# Test the function\n",
    "test_tokens = ['h', 'e', 'l', 'l', 'o']\n",
    "result = merge_pair(test_tokens, ('l', 'l'), 'll')\n",
    "print(\"Test input:\", test_tokens)\n",
    "print(\"After merging ('l', 'l') -> 'll':\", result)\n",
    "print(\"Expected: ['h', 'e', 'll', 'o']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Main BPE Training Loop - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(text: str, vocab_size: int, verbose: bool = True) -> Tuple[List[Tuple[str, str]], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Train a BPE tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        text: Training corpus\n",
    "        vocab_size: Target vocabulary size\n",
    "        verbose: Print merge operations\n",
    "        \n",
    "    Returns:\n",
    "        - List of merge operations in order\n",
    "        - Final vocabulary with token frequencies\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize with character-level tokens\n",
    "    tokens = list(text)\n",
    "    vocab = Counter(tokens)\n",
    "    merges = []\n",
    "    \n",
    "    initial_vocab_size = len(vocab)\n",
    "    \n",
    "    # Step 2: Perform merges until vocab_size is reached\n",
    "    while len(vocab) < vocab_size:\n",
    "        # 2a. Get pair frequencies\n",
    "        pairs = get_pair_frequencies(tokens)\n",
    "        \n",
    "        if not pairs:\n",
    "            break\n",
    "        \n",
    "        # 2b. Find most frequent pair\n",
    "        most_frequent_pair = max(pairs.items(), key=lambda x: x[1])\n",
    "        pair, freq = most_frequent_pair\n",
    "        \n",
    "        # 2c. Create new token\n",
    "        new_token = pair[0] + pair[1]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Merge #{len(merges) + 1}: {pair} -> '{new_token}' (freq: {freq})\")\n",
    "        \n",
    "        # 2d. Merge the pair in token list\n",
    "        tokens = merge_pair(tokens, pair, new_token)\n",
    "        \n",
    "        # 2e. Update vocabulary\n",
    "        vocab[new_token] = freq\n",
    "        \n",
    "        # 2f. Record the merge\n",
    "        merges.append(pair)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nInitial vocabulary size: {initial_vocab_size}\")\n",
    "        print(f\"Final vocabulary size: {len(vocab)}\")\n",
    "        print(f\"Number of merges: {len(merges)}\")\n",
    "    \n",
    "    return merges, vocab\n",
    "\n",
    "# Train on sample text\n",
    "merges, vocab = train_bpe(SAMPLE_TEXT, vocab_size=300, verbose=True)\n",
    "print(f\"\\nTop 10 tokens by frequency:\")\n",
    "for token, freq in sorted(vocab.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"  '{token}': {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Encode Text with Learned Merges - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str, merges: List[Tuple[str, str]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize text using learned BPE merges.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to tokenize\n",
    "        merges: Ordered list of merge operations\n",
    "        \n",
    "    Returns:\n",
    "        List of tokens\n",
    "    \"\"\"\n",
    "    # Start with character-level tokens\n",
    "    tokens = list(text)\n",
    "    \n",
    "    # Apply each merge in order\n",
    "    for pair in merges:\n",
    "        new_token = pair[0] + pair[1]\n",
    "        tokens = merge_pair(tokens, pair, new_token)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Test encoding\n",
    "test_sentence = \"The quick fox jumps\"\n",
    "encoded = encode(test_sentence, merges)\n",
    "print(f\"Original: {test_sentence}\")\n",
    "print(f\"Tokens: {encoded}\")\n",
    "print(f\"Compression: {len(test_sentence)} chars -> {len(encoded)} tokens\")\n",
    "print(f\"Compression ratio: {len(test_sentence) / len(encoded):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Visualization\n",
    "\n",
    "### Task 5: Color-Coded Token Visualizer - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tokens(text: str, tokens: List[str]):\n",
    "    \"\"\"\n",
    "    Create a color-coded visualization of tokens.\n",
    "    \"\"\"\n",
    "    # Generate distinct colors for each token\n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(tokens)))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 3))\n",
    "    x_pos = 0\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        # Plot each token as a colored rectangle\n",
    "        width = len(token)\n",
    "        ax.barh(0, width, left=x_pos, color=colors[i], edgecolor='black', height=0.5)\n",
    "        \n",
    "        # Add token text\n",
    "        # Replace special characters for display\n",
    "        display_token = token.replace('\\n', '\\\\n').replace(' ', 'â£')\n",
    "        ax.text(x_pos + width/2, 0, display_token, \n",
    "               ha='center', va='center', fontsize=8, weight='bold')\n",
    "        \n",
    "        x_pos += width\n",
    "    \n",
    "    ax.set_xlim(0, x_pos)\n",
    "    ax.set_ylim(-0.5, 0.5)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f'Token Visualization: \"{text}\" -> {len(tokens)} tokens', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the encoded sentence\n",
    "visualize_tokens(test_sentence, encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Ablation Study\n",
    "\n",
    "### Task 6: Cross-Language Tokenization - SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinese text sample\n",
    "CHINESE_TEXT = \"ä½ å¥½ä¸–ç•Œã€‚è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ã€‚æœºå™¨å­¦ä¹ å¾ˆæœ‰è¶£ã€‚\"\n",
    "\n",
    "def measure_efficiency(text: str, tokens: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Measure tokenization efficiency metrics.\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'compression_ratio': len(text) / len(tokens),\n",
    "        'avg_token_length': sum(len(t) for t in tokens) / len(tokens),\n",
    "        'single_char_tokens': sum(1 for t in tokens if len(t) == 1) / len(tokens) * 100\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Encode Chinese text with English-trained tokenizer\n",
    "chinese_tokens_english = encode(CHINESE_TEXT, merges)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"English tokenizer on Chinese text:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Text: {CHINESE_TEXT}\")\n",
    "print(f\"Tokens: {chinese_tokens_english[:20]}...\")  # Show first 20\n",
    "print(f\"Total tokens: {len(chinese_tokens_english)}\")\n",
    "metrics_eng = measure_efficiency(CHINESE_TEXT, chinese_tokens_english)\n",
    "print(f\"\\nMetrics:\")\n",
    "for key, value in metrics_eng.items():\n",
    "    print(f\"  {key}: {value:.2f}\")\n",
    "\n",
    "# Train a new tokenizer on Chinese\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training tokenizer on Chinese text...\")\n",
    "print(\"=\"*60)\n",
    "chinese_merges, chinese_vocab = train_bpe(CHINESE_TEXT, vocab_size=300, verbose=False)\n",
    "chinese_tokens_native = encode(CHINESE_TEXT, chinese_merges)\n",
    "\n",
    "print(\"\\nChinese tokenizer on Chinese text:\")\n",
    "print(f\"Text: {CHINESE_TEXT}\")\n",
    "print(f\"Tokens: {chinese_tokens_native}\")\n",
    "print(f\"Total tokens: {len(chinese_tokens_native)}\")\n",
    "metrics_chi = measure_efficiency(CHINESE_TEXT, chinese_tokens_native)\n",
    "print(f\"\\nMetrics:\")\n",
    "for key, value in metrics_chi.items():\n",
    "    print(f\"  {key}: {value:.2f}\")\n",
    "\n",
    "# Comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Token count - English tokenizer: {len(chinese_tokens_english)}\")\n",
    "print(f\"Token count - Chinese tokenizer: {len(chinese_tokens_native)}\")\n",
    "print(f\"Efficiency improvement: {len(chinese_tokens_english) / len(chinese_tokens_native):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Analysis and Conclusions\n",
    "\n",
    "### Questions and Answers:\n",
    "\n",
    "#### 1. What patterns did you observe in the merge order?\n",
    "\n",
    "**Answer:** \n",
    "- Common character pairs merge first (e.g., 'th', 'he', 'in')\n",
    "- Frequent words become single tokens (e.g., 'the', 'and')\n",
    "- Space + common letter combinations merge early\n",
    "- The algorithm is purely statistical - it has no linguistic knowledge\n",
    "\n",
    "#### 2. How did the English tokenizer perform on Chinese text?\n",
    "\n",
    "**Answer:**\n",
    "- Very poorly! Each Chinese character remained a separate token\n",
    "- High single-character token percentage (~100%)\n",
    "- Low compression ratio (close to 1:1)\n",
    "- This demonstrates why multilingual models need much larger vocabularies\n",
    "\n",
    "#### 3. What is the relationship between vocabulary size and compression ratio?\n",
    "\n",
    "**Answer:**\n",
    "- Larger vocabulary â†’ more merges â†’ longer tokens â†’ better compression\n",
    "- Trade-off: Larger vocab = larger embedding matrix = more parameters\n",
    "- Diminishing returns after a certain point\n",
    "- Typical LLMs use 32k-100k vocab size as a balance\n",
    "\n",
    "#### 4. Why do multilingual models like mBERT need 100k+ vocabulary size?\n",
    "\n",
    "**Answer:**\n",
    "- Each language has unique character combinations\n",
    "- Need to efficiently represent characters from multiple scripts (Latin, Cyrillic, Chinese, Arabic, etc.)\n",
    "- Without large vocab, some languages get over-segmented (inefficient)\n",
    "- 100k+ allows reasonable compression across many languages simultaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Additional Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot vocabulary growth\n",
    "def plot_vocab_growth():\n",
    "    \"\"\"Track vocabulary size over merge iterations.\"\"\"\n",
    "    sizes = [50, 100, 150, 200, 250, 300]\n",
    "    results = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        merges_temp, vocab_temp = train_bpe(SAMPLE_TEXT, vocab_size=size, verbose=False)\n",
    "        test_encoded = encode(test_sentence, merges_temp)\n",
    "        results.append({\n",
    "            'vocab_size': len(vocab_temp),\n",
    "            'num_tokens': len(test_encoded),\n",
    "            'compression': len(test_sentence) / len(test_encoded)\n",
    "        })\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Vocabulary size vs target\n",
    "    ax1.plot(sizes, [r['vocab_size'] for r in results], 'o-')\n",
    "    ax1.set_xlabel('Target Vocabulary Size')\n",
    "    ax1.set_ylabel('Actual Vocabulary Size')\n",
    "    ax1.set_title('Vocabulary Growth')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Compression ratio\n",
    "    ax2.plot(sizes, [r['compression'] for r in results], 'o-', color='green')\n",
    "    ax2.set_xlabel('Vocabulary Size')\n",
    "    ax2.set_ylabel('Compression Ratio')\n",
    "    ax2.set_title('Compression vs Vocabulary Size')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_vocab_growth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Completion Checklist\n",
    "\n",
    "- âœ… Implemented `get_pair_frequencies()`\n",
    "- âœ… Implemented `merge_pair()`\n",
    "- âœ… Implemented `train_bpe()`\n",
    "- âœ… Implemented `encode()`\n",
    "- âœ… Created token visualization\n",
    "- âœ… Performed cross-language ablation study\n",
    "- âœ… Analyzed efficiency metrics\n",
    "- âœ… Answered reflection questions\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **BPE is purely statistical** - No linguistic knowledge, just frequency\n",
    "2. **Language-specific tokenizers are more efficient** - Trained on target language\n",
    "3. **Vocabulary size is a trade-off** - Compression vs model size\n",
    "4. **Multilingual = larger vocab** - Need to cover many character combinations\n",
    "\n",
    "## ðŸš€ Next Project\n",
    "Move to **02_meaning_space** to learn how these discrete tokens become continuous vector representations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
