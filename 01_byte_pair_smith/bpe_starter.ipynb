{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: The Tokenizer Smith\n",
    "## Building Byte-Pair Encoding from Scratch\n",
    "\n",
    "**Goal:** Implement BPE tokenization without using `transformers` or `tokenizers` libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Sample corpus for training\n",
    "SAMPLE_TEXT = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog.\n",
    "The dog was not amused by the fox's behavior.\n",
    "Jumping quickly, the fox escaped into the forest.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Corpus length: {len(SAMPLE_TEXT)} characters\")\n",
    "print(f\"Unique characters: {len(set(SAMPLE_TEXT))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implement BPE Core Functions\n",
    "\n",
    "### Task 1: Get Pair Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_frequencies(tokens: List[str]) -> Dict[Tuple[str, str], int]:\n",
    "    \"\"\"\n",
    "    Count the frequency of all adjacent token pairs.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of current tokens\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping (token1, token2) -> frequency\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pairs = defaultdict(int)\n",
    "    \n",
    "    # Hint: Iterate through adjacent pairs and count them\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Test your function\n",
    "test_tokens = ['h', 'e', 'l', 'l', 'o']\n",
    "print(get_pair_frequencies(test_tokens))\n",
    "# Expected: {('h', 'e'): 1, ('e', 'l'): 1, ('l', 'l'): 1, ('l', 'o'): 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Merge a Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pair(tokens: List[str], pair: Tuple[str, str], new_token: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Replace all occurrences of a pair with a new merged token.\n",
    "    \n",
    "    Args:\n",
    "        tokens: Current token list\n",
    "        pair: The pair to merge (token1, token2)\n",
    "        new_token: The merged token (token1 + token2)\n",
    "        \n",
    "    Returns:\n",
    "        New token list with pairs merged\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    new_tokens = []\n",
    "    \n",
    "    # Hint: Iterate through tokens, and when you find the pair, merge them\n",
    "    \n",
    "    return new_tokens\n",
    "\n",
    "# Test your function\n",
    "test_tokens = ['h', 'e', 'l', 'l', 'o']\n",
    "result = merge_pair(test_tokens, ('l', 'l'), 'll')\n",
    "print(result)\n",
    "# Expected: ['h', 'e', 'll', 'o']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Main BPE Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(text: str, vocab_size: int, verbose: bool = True) -> Tuple[List[Tuple[str, str]], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Train a BPE tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        text: Training corpus\n",
    "        vocab_size: Target vocabulary size\n",
    "        verbose: Print merge operations\n",
    "        \n",
    "    Returns:\n",
    "        - List of merge operations in order\n",
    "        - Final vocabulary with token frequencies\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize with character-level tokens\n",
    "    tokens = list(text)\n",
    "    vocab = Counter(tokens)\n",
    "    merges = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    # Step 2: Perform merges until vocab_size is reached\n",
    "    while len(vocab) < vocab_size:\n",
    "        # 2a. Get pair frequencies\n",
    "        \n",
    "        # 2b. Find most frequent pair\n",
    "        \n",
    "        # 2c. Merge the pair\n",
    "        \n",
    "        # 2d. Update vocabulary\n",
    "        \n",
    "        # 2e. Record the merge\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    return merges, vocab\n",
    "\n",
    "# Train on sample text\n",
    "merges, vocab = train_bpe(SAMPLE_TEXT, vocab_size=300, verbose=True)\n",
    "print(f\"\\nFinal vocabulary size: {len(vocab)}\")\n",
    "print(f\"Number of merges: {len(merges)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Encode Text with Learned Merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text: str, merges: List[Tuple[str, str]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize text using learned BPE merges.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to tokenize\n",
    "        merges: Ordered list of merge operations\n",
    "        \n",
    "    Returns:\n",
    "        List of tokens\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    tokens = list(text)\n",
    "    \n",
    "    # Apply each merge in order\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Test encoding\n",
    "test_sentence = \"The quick fox jumps\"\n",
    "encoded = encode(test_sentence, merges)\n",
    "print(f\"Original: {test_sentence}\")\n",
    "print(f\"Tokens: {encoded}\")\n",
    "print(f\"Compression: {len(test_sentence)} chars -> {len(encoded)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Visualization\n",
    "\n",
    "### Task 5: Color-Coded Token Visualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tokens(text: str, tokens: List[str]):\n",
    "    \"\"\"\n",
    "    Create a color-coded visualization of tokens.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    # Option 1: Generate HTML with colored spans\n",
    "    # Option 2: Use matplotlib to create a colored bar chart\n",
    "    \n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(tokens)))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 2))\n",
    "    x_pos = 0\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        # Plot each token as a colored rectangle\n",
    "        pass\n",
    "    \n",
    "    plt.title(\"Token Visualization\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the encoded sentence\n",
    "visualize_tokens(test_sentence, encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Ablation Study\n",
    "\n",
    "### Task 6: Cross-Language Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinese text sample\n",
    "CHINESE_TEXT = \"ä½ å¥½ä¸–ç•Œã€‚è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ã€‚æœºå™¨å­¦ä¹ å¾ˆæœ‰è¶£ã€‚\"\n",
    "\n",
    "def measure_efficiency(text: str, tokens: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Measure tokenization efficiency metrics.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    metrics = {\n",
    "        'compression_ratio': len(text) / len(tokens),\n",
    "        'avg_token_length': sum(len(t) for t in tokens) / len(tokens),\n",
    "        'single_char_tokens': sum(1 for t in tokens if len(t) == 1) / len(tokens)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Encode Chinese text with English-trained tokenizer\n",
    "chinese_tokens = encode(CHINESE_TEXT, merges)\n",
    "\n",
    "print(\"English tokenizer on Chinese text:\")\n",
    "print(f\"Tokens: {chinese_tokens}\")\n",
    "print(f\"Metrics: {measure_efficiency(CHINESE_TEXT, chinese_tokens)}\")\n",
    "\n",
    "# Train a new tokenizer on Chinese\n",
    "chinese_merges, chinese_vocab = train_bpe(CHINESE_TEXT, vocab_size=300, verbose=False)\n",
    "chinese_tokens_native = encode(CHINESE_TEXT, chinese_merges)\n",
    "\n",
    "print(\"\\nChinese tokenizer on Chinese text:\")\n",
    "print(f\"Tokens: {chinese_tokens_native}\")\n",
    "print(f\"Metrics: {measure_efficiency(CHINESE_TEXT, chinese_tokens_native)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Analysis and Conclusions\n",
    "\n",
    "### Questions to Answer:\n",
    "\n",
    "1. **What patterns did you observe in the merge order?**\n",
    "   - YOUR ANSWER HERE\n",
    "\n",
    "2. **How did the English tokenizer perform on Chinese text?**\n",
    "   - YOUR ANSWER HERE\n",
    "\n",
    "3. **What is the relationship between vocabulary size and compression ratio?**\n",
    "   - YOUR ANSWER HERE\n",
    "\n",
    "4. **Why do multilingual models like mBERT need 100k+ vocabulary size?**\n",
    "   - YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Completion Checklist\n",
    "\n",
    "- [ ] Implemented `get_pair_frequencies()`\n",
    "- [ ] Implemented `merge_pair()`\n",
    "- [ ] Implemented `train_bpe()`\n",
    "- [ ] Implemented `encode()`\n",
    "- [ ] Created token visualization\n",
    "- [ ] Performed cross-language ablation study\n",
    "- [ ] Analyzed efficiency metrics\n",
    "- [ ] Answered reflection questions\n",
    "\n",
    "## ðŸš€ Next Project\n",
    "Move to **02_meaning_space** to learn how these discrete tokens become continuous vector representations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
